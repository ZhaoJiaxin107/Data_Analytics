{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>cnt</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>hum</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_code</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/1/4 0:00</td>\n",
       "      <td>182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015/1/4 1:00</td>\n",
       "      <td>138</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015/1/4 2:00</td>\n",
       "      <td>134</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>96.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015/1/4 3:00</td>\n",
       "      <td>72</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015/1/4 4:00</td>\n",
       "      <td>47</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015/1/4 5:00</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015/1/4 6:00</td>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015/1/4 7:00</td>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015/1/4 8:00</td>\n",
       "      <td>131</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015/1/4 9:00</td>\n",
       "      <td>301</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2015/1/4 10:00</td>\n",
       "      <td>528</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2015/1/4 11:00</td>\n",
       "      <td>727</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2015/1/4 12:00</td>\n",
       "      <td>862</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>96.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015/1/4 13:00</td>\n",
       "      <td>916</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015/1/4 14:00</td>\n",
       "      <td>1039</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015/1/4 15:00</td>\n",
       "      <td>869</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2015/1/4 16:00</td>\n",
       "      <td>737</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2015/1/4 17:00</td>\n",
       "      <td>594</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2015/1/4 18:00</td>\n",
       "      <td>522</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2015/1/4 19:00</td>\n",
       "      <td>379</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2015/1/4 20:00</td>\n",
       "      <td>328</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2015/1/4 21:00</td>\n",
       "      <td>221</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp   cnt   t1   t2    hum  wind_speed  weather_code  \\\n",
       "0    2015/1/4 0:00   182  3.0  2.0   93.0         6.0             3   \n",
       "1    2015/1/4 1:00   138  3.0  2.5   93.0         5.0             1   \n",
       "2    2015/1/4 2:00   134  2.5  2.5   96.5         0.0             1   \n",
       "3    2015/1/4 3:00    72  2.0  2.0  100.0         0.0             1   \n",
       "4    2015/1/4 4:00    47  2.0  0.0   93.0         6.5             1   \n",
       "5    2015/1/4 5:00    46  2.0  2.0   93.0         4.0             1   \n",
       "6    2015/1/4 6:00    51  1.0 -1.0  100.0         7.0             4   \n",
       "7    2015/1/4 7:00    75  1.0 -1.0  100.0         7.0             4   \n",
       "8    2015/1/4 8:00   131  1.5 -1.0   96.5         8.0             4   \n",
       "9    2015/1/4 9:00   301  2.0 -0.5  100.0         9.0             3   \n",
       "10  2015/1/4 10:00   528  3.0 -0.5   93.0        12.0             3   \n",
       "11  2015/1/4 11:00   727  2.0 -1.5  100.0        12.0             3   \n",
       "12  2015/1/4 12:00   862  2.0 -1.5   96.5        13.0             4   \n",
       "13  2015/1/4 13:00   916  3.0 -0.5   87.0        15.0             3   \n",
       "14  2015/1/4 14:00  1039  2.5  0.0   90.0         8.0             3   \n",
       "15  2015/1/4 15:00   869  2.0 -1.5   93.0        11.0             3   \n",
       "16  2015/1/4 16:00   737  3.0  0.0   93.0        12.0             3   \n",
       "17  2015/1/4 17:00   594  3.0  0.0   93.0        11.0             3   \n",
       "18  2015/1/4 18:00   522  3.0  1.5   93.0         6.5             3   \n",
       "19  2015/1/4 19:00   379  3.0  1.0   93.0         7.0             3   \n",
       "20  2015/1/4 20:00   328  3.0  3.0   93.0         4.0             3   \n",
       "21  2015/1/4 21:00   221  3.0  2.5   93.0         5.0             4   \n",
       "\n",
       "    is_holiday  is_weekend  season  \n",
       "0            0           1       3  \n",
       "1            0           1       3  \n",
       "2            0           1       3  \n",
       "3            0           1       3  \n",
       "4            0           1       3  \n",
       "5            0           1       3  \n",
       "6            0           1       3  \n",
       "7            0           1       3  \n",
       "8            0           1       3  \n",
       "9            0           1       3  \n",
       "10           0           1       3  \n",
       "11           0           1       3  \n",
       "12           0           1       3  \n",
       "13           0           1       3  \n",
       "14           0           1       3  \n",
       "15           0           1       3  \n",
       "16           0           1       3  \n",
       "17           0           1       3  \n",
       "18           0           1       3  \n",
       "19           0           1       3  \n",
       "20           0           1       3  \n",
       "21           0           1       3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('C:/Users/lenovo/python_code/london_merged.csv')\n",
    "dataset.head(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>hum</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_code</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>96.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    t1   t2    hum  wind_speed  weather_code  is_holiday  is_weekend  season\n",
       "0  3.0  2.0   93.0         6.0             3           0           1       3\n",
       "1  3.0  2.5   93.0         5.0             1           0           1       3\n",
       "2  2.5  2.5   96.5         0.0             1           0           1       3\n",
       "3  2.0  2.0  100.0         0.0             1           0           1       3\n",
       "4  2.0  0.0   93.0         6.5             1           0           1       3\n",
       "5  2.0  2.0   93.0         4.0             1           0           1       3\n",
       "6  1.0 -1.0  100.0         7.0             4           0           1       3\n",
       "7  1.0 -1.0  100.0         7.0             4           0           1       3\n",
       "8  1.5 -1.0   96.5         8.0             4           0           1       3\n",
       "9  2.0 -0.5  100.0         9.0             3           0           1       3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dataset.iloc[:,2:10]\n",
    "x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    182\n",
       "1    138\n",
       "2    134\n",
       "3     72\n",
       "4     47\n",
       "5     46\n",
       "6     51\n",
       "7     75\n",
       "8    131\n",
       "9    301\n",
       "Name: cnt, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset.iloc[:,1]\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py:61: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "y = np.reshape(y,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "print(scaler_x.fit(x))\n",
    "xscale = scaler_x.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "print(scaler_y.fit(y))\n",
    "yscale = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(xscale, yscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 24)                216       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 33)                825       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 34        \n",
      "=================================================================\n",
      "Total params: 1,287\n",
      "Trainable params: 1,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(24, activation='sigmoid'))\n",
    "model.add(Dense(33, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10448 samples, validate on 2612 samples\n",
      "Epoch 1/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 1s 65us/step - loss: 0.0213 - mean_squared_error: 0.0213 - mean_absolute_error: 0.1112 - val_loss: 0.0139 - val_mean_squared_error: 0.0139 - val_mean_absolute_error: 0.0890\n",
      "\n",
      "Epoch 2/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 41us/step - loss: 0.0147 - mean_squared_error: 0.0147 - mean_absolute_error: 0.0914 - val_loss: 0.0128 - val_mean_squared_error: 0.0128 - val_mean_absolute_error: 0.0813\n",
      "\n",
      "Epoch 3/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 40us/step - loss: 0.0144 - mean_squared_error: 0.0144 - mean_absolute_error: 0.0893 - val_loss: 0.0125 - val_mean_squared_error: 0.0125 - val_mean_absolute_error: 0.0833\n",
      "\n",
      "Epoch 4/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 40us/step - loss: 0.0143 - mean_squared_error: 0.0143 - mean_absolute_error: 0.0888 - val_loss: 0.0127 - val_mean_squared_error: 0.0127 - val_mean_absolute_error: 0.0796\n",
      "\n",
      "Epoch 5/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 41us/step - loss: 0.0142 - mean_squared_error: 0.0142 - mean_absolute_error: 0.0883 - val_loss: 0.0125 - val_mean_squared_error: 0.0125 - val_mean_absolute_error: 0.0857\n",
      "\n",
      "Epoch 6/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0142 - mean_squared_error: 0.0142 - mean_absolute_error: 0.0884 - val_loss: 0.0123 - val_mean_squared_error: 0.0123 - val_mean_absolute_error: 0.0831\n",
      "\n",
      "Epoch 7/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0142 - mean_squared_error: 0.0142 - mean_absolute_error: 0.0880 - val_loss: 0.0123 - val_mean_squared_error: 0.0123 - val_mean_absolute_error: 0.0799\n",
      "\n",
      "Epoch 8/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0143 - mean_squared_error: 0.0143 - mean_absolute_error: 0.0885 - val_loss: 0.0123 - val_mean_squared_error: 0.0123 - val_mean_absolute_error: 0.0845\n",
      "\n",
      "Epoch 9/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 40us/step - loss: 0.0141 - mean_squared_error: 0.0141 - mean_absolute_error: 0.0878 - val_loss: 0.0125 - val_mean_squared_error: 0.0125 - val_mean_absolute_error: 0.0870\n",
      "\n",
      "Epoch 10/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 40us/step - loss: 0.0142 - mean_squared_error: 0.0142 - mean_absolute_error: 0.0879 - val_loss: 0.0122 - val_mean_squared_error: 0.0122 - val_mean_absolute_error: 0.0793\n",
      "\n",
      "Epoch 11/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 38us/step - loss: 0.0141 - mean_squared_error: 0.0141 - mean_absolute_error: 0.0877 - val_loss: 0.0122 - val_mean_squared_error: 0.0122 - val_mean_absolute_error: 0.0790\n",
      "\n",
      "Epoch 12/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0141 - mean_squared_error: 0.0141 - mean_absolute_error: 0.0876 - val_loss: 0.0124 - val_mean_squared_error: 0.0124 - val_mean_absolute_error: 0.0781\n",
      "\n",
      "Epoch 13/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0140 - mean_squared_error: 0.0140 - mean_absolute_error: 0.0874 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0804\n",
      "\n",
      "Epoch 14/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 38us/step - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0871 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0793\n",
      "\n",
      "Epoch 15/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0141 - mean_squared_error: 0.0141 - mean_absolute_error: 0.0872 - val_loss: 0.0123 - val_mean_squared_error: 0.0123 - val_mean_absolute_error: 0.0852\n",
      "\n",
      "Epoch 16/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0141 - mean_squared_error: 0.0141 - mean_absolute_error: 0.0874 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0809\n",
      "\n",
      "Epoch 17/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 33us/step - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0867 - val_loss: 0.0129 - val_mean_squared_error: 0.0129 - val_mean_absolute_error: 0.0893\n",
      "\n",
      "Epoch 18/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0140 - mean_squared_error: 0.0140 - mean_absolute_error: 0.0874 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0819\n",
      "\n",
      "Epoch 19/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0866 - val_loss: 0.0120 - val_mean_squared_error: 0.0120 - val_mean_absolute_error: 0.0794\n",
      "\n",
      "Epoch 20/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0140 - mean_squared_error: 0.0140 - mean_absolute_error: 0.0869 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0820\n",
      "\n",
      "Epoch 21/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0140 - mean_squared_error: 0.0140 - mean_absolute_error: 0.0868 - val_loss: 0.0124 - val_mean_squared_error: 0.0124 - val_mean_absolute_error: 0.0858\n",
      "\n",
      "Epoch 22/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0866 - val_loss: 0.0122 - val_mean_squared_error: 0.0122 - val_mean_absolute_error: 0.0838\n",
      "\n",
      "Epoch 23/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0140 - mean_squared_error: 0.0140 - mean_absolute_error: 0.0871 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0830\n",
      "\n",
      "Epoch 24/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0864 - val_loss: 0.0120 - val_mean_squared_error: 0.0120 - val_mean_absolute_error: 0.0783\n",
      "\n",
      "Epoch 25/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 38us/step - loss: 0.0140 - mean_squared_error: 0.0140 - mean_absolute_error: 0.0869 - val_loss: 0.0125 - val_mean_squared_error: 0.0125 - val_mean_absolute_error: 0.0872\n",
      "\n",
      "Epoch 26/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0140 - mean_squared_error: 0.0140 - mean_absolute_error: 0.0869 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0834\n",
      "\n",
      "Epoch 27/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0140 - mean_squared_error: 0.0140 - mean_absolute_error: 0.0870 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0782\n",
      "\n",
      "Epoch 28/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 40us/step - loss: 0.0140 - mean_squared_error: 0.0140 - mean_absolute_error: 0.0867 - val_loss: 0.0123 - val_mean_squared_error: 0.0123 - val_mean_absolute_error: 0.0852\n",
      "\n",
      "Epoch 29/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0866 - val_loss: 0.0123 - val_mean_squared_error: 0.0123 - val_mean_absolute_error: 0.0767\n",
      "\n",
      "Epoch 30/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 32us/step - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0864 - val_loss: 0.0128 - val_mean_squared_error: 0.0128 - val_mean_absolute_error: 0.0771\n",
      "\n",
      "Epoch 31/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 38us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0863 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0799\n",
      "\n",
      "Epoch 32/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0862 - val_loss: 0.0120 - val_mean_squared_error: 0.0120 - val_mean_absolute_error: 0.0828\n",
      "\n",
      "Epoch 33/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0870 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0799\n",
      "\n",
      "Epoch 34/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 45us/step - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0862 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0790\n",
      "\n",
      "Epoch 35/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0858 - val_loss: 0.0123 - val_mean_squared_error: 0.0123 - val_mean_absolute_error: 0.0766\n",
      "\n",
      "Epoch 36/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0860 - val_loss: 0.0120 - val_mean_squared_error: 0.0120 - val_mean_absolute_error: 0.0816\n",
      "\n",
      "Epoch 37/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0859 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0805\n",
      "\n",
      "Epoch 38/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0860 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0791\n",
      "\n",
      "Epoch 39/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0857 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0813\n",
      "\n",
      "Epoch 40/150\n",
      "10448/10448 [==============================]10100/10448 [============================>.] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0810448/10448 [==============================] - 0s 44us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0858 - val_loss: 0.0124 - val_mean_squared_error: 0.0124 - val_mean_absolute_error: 0.0761\n",
      "\n",
      "Epoch 41/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 47us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0859 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0839\n",
      "\n",
      "Epoch 42/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 45us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0860 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0791\n",
      "\n",
      "Epoch 43/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0859 - val_loss: 0.0120 - val_mean_squared_error: 0.0120 - val_mean_absolute_error: 0.0769\n",
      "\n",
      "Epoch 44/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0859 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0796\n",
      "\n",
      "Epoch 45/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 48us/step - loss: 0.0140 - mean_squared_error: 0.0140 - mean_absolute_error: 0.0866 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0841\n",
      "\n",
      "Epoch 46/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 1s 51us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0859 - val_loss: 0.0120 - val_mean_squared_error: 0.0120 - val_mean_absolute_error: 0.0819\n",
      "\n",
      "Epoch 47/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 1s 49us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0857 - val_loss: 0.0123 - val_mean_squared_error: 0.0123 - val_mean_absolute_error: 0.0855\n",
      "\n",
      "Epoch 48/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 46us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0857 - val_loss: 0.0120 - val_mean_squared_error: 0.0120 - val_mean_absolute_error: 0.0775\n",
      "\n",
      "Epoch 49/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0855 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0799\n",
      "\n",
      "Epoch 50/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 46us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0857 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0774\n",
      "\n",
      "Epoch 51/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0864 - val_loss: 0.0126 - val_mean_squared_error: 0.0126 - val_mean_absolute_error: 0.0880\n",
      "\n",
      "Epoch 52/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0857 - val_loss: 0.0128 - val_mean_squared_error: 0.0128 - val_mean_absolute_error: 0.0900\n",
      "\n",
      "Epoch 53/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0858 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0786\n",
      "\n",
      "Epoch 54/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 40us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0856 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0802\n",
      "\n",
      "Epoch 55/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0854 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0839\n",
      "\n",
      "Epoch 56/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0855 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0807\n",
      "\n",
      "Epoch 57/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0855 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0801\n",
      "\n",
      "Epoch 58/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 41us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0859 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0788\n",
      "\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0855 - val_loss: 0.0120 - val_mean_squared_error: 0.0120 - val_mean_absolute_error: 0.0830\n",
      "\n",
      "Epoch 60/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 41us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0858 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0771\n",
      "\n",
      "Epoch 61/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0851 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0784\n",
      "\n",
      "Epoch 62/150\n",
      "10448/10448 [==============================]10000/10448 [===========================>..] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0810448/10448 [==============================] - 0s 34us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0854 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0777\n",
      "\n",
      "Epoch 63/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0859 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0789\n",
      "\n",
      "Epoch 64/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0853 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0793\n",
      "\n",
      "Epoch 65/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0854 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0812\n",
      "\n",
      "Epoch 66/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 42us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0851 - val_loss: 0.0124 - val_mean_squared_error: 0.0124 - val_mean_absolute_error: 0.0869\n",
      "\n",
      "Epoch 67/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0854 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0775\n",
      "\n",
      "Epoch 68/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0138 - mean_squared_error: 0.0138 - mean_absolute_error: 0.0859 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0773\n",
      "\n",
      "Epoch 69/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.08 - 0s 39us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0852 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0761\n",
      "\n",
      "Epoch 70/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0852 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0811\n",
      "\n",
      "Epoch 71/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0854 - val_loss: 0.0120 - val_mean_squared_error: 0.0120 - val_mean_absolute_error: 0.0825\n",
      "\n",
      "Epoch 72/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0850 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0773\n",
      "\n",
      "Epoch 73/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0849 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0802\n",
      "\n",
      "Epoch 74/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0850 - val_loss: 0.0122 - val_mean_squared_error: 0.0122 - val_mean_absolute_error: 0.0857\n",
      "\n",
      "Epoch 75/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0854 - val_loss: 0.0129 - val_mean_squared_error: 0.0129 - val_mean_absolute_error: 0.0911\n",
      "\n",
      "Epoch 76/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0855 - val_loss: 0.0122 - val_mean_squared_error: 0.0122 - val_mean_absolute_error: 0.0754\n",
      "\n",
      "Epoch 77/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0137 - mean_squared_error: 0.0137 - mean_absolute_error: 0.0856 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0781\n",
      "\n",
      "Epoch 78/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0851 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0765\n",
      "\n",
      "Epoch 79/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0848 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0788\n",
      "\n",
      "Epoch 80/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0849 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0798\n",
      "\n",
      "Epoch 81/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0851 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0809\n",
      "\n",
      "Epoch 82/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0853 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0811\n",
      "\n",
      "Epoch 83/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0849 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0802\n",
      "\n",
      "Epoch 84/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0848 - val_loss: 0.0123 - val_mean_squared_error: 0.0123 - val_mean_absolute_error: 0.0861\n",
      "\n",
      "Epoch 85/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0852 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0758\n",
      "\n",
      "Epoch 86/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0846 - val_loss: 0.0136 - val_mean_squared_error: 0.0136 - val_mean_absolute_error: 0.0957\n",
      "\n",
      "Epoch 87/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0848 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 32us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0847 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0808\n",
      "\n",
      "Epoch 89/150\n",
      "10448/10448 [==============================]10350/10448 [============================>.] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0810448/10448 [==============================] - 0s 34us/step - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0849 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0775\n",
      "\n",
      "Epoch 90/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0846 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0792\n",
      "\n",
      "Epoch 91/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0848 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0820\n",
      "\n",
      "Epoch 92/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0848 - val_loss: 0.0120 - val_mean_squared_error: 0.0120 - val_mean_absolute_error: 0.0750\n",
      "\n",
      "Epoch 93/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 32us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0843 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0848\n",
      "\n",
      "Epoch 94/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0845 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0772.0134 - mean_absolute_error: 0.\n",
      "\n",
      "Epoch 95/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 30us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0847 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0777\n",
      "\n",
      "Epoch 96/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0844 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0784\n",
      "\n",
      "Epoch 97/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 44us/step - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0843 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0780\n",
      "\n",
      "Epoch 98/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0843 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0772\n",
      "\n",
      "Epoch 99/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0848 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0756\n",
      "\n",
      "Epoch 100/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0842 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0788\n",
      "\n",
      "Epoch 101/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0842 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0783\n",
      "\n",
      "Epoch 102/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0848 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0796\n",
      "\n",
      "Epoch 103/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0841 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0833\n",
      "\n",
      "Epoch 104/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 40us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0840 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0821\n",
      "\n",
      "Epoch 105/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0842 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0802\n",
      "\n",
      "Epoch 106/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0839 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0755\n",
      "\n",
      "Epoch 107/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0843 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0826\n",
      "\n",
      "Epoch 108/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0841 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0797\n",
      "\n",
      "Epoch 109/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 40us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0840 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0823\n",
      "\n",
      "Epoch 110/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0840 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0764\n",
      "\n",
      "Epoch 111/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0840 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0787\n",
      "\n",
      "Epoch 112/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0841 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0770\n",
      "\n",
      "Epoch 113/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0837 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0809\n",
      "\n",
      "Epoch 114/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0835 - val_loss: 0.0118 - val_mean_squared_error: 0.0118 - val_mean_absolute_error: 0.0831\n",
      "\n",
      "Epoch 115/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0837 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0773\n",
      "\n",
      "Epoch 116/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0840 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 117/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0840 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0794\n",
      "\n",
      "Epoch 118/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0836 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0769\n",
      "\n",
      "Epoch 119/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0839 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0794\n",
      "\n",
      "Epoch 120/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0840 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0781\n",
      "\n",
      "Epoch 121/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0836 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0782\n",
      "\n",
      "Epoch 122/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 38us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0841 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0751\n",
      "\n",
      "Epoch 123/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0838 - val_loss: 0.0121 - val_mean_squared_error: 0.0121 - val_mean_absolute_error: 0.0862\n",
      "\n",
      "Epoch 124/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 32us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0836 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0749\n",
      "\n",
      "Epoch 125/150\n",
      "10448/10448 [==============================]10200/10448 [============================>.] - ETA: 0s - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0810448/10448 [==============================] - 0s 34us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0838 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0775\n",
      "\n",
      "Epoch 126/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0833 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0767\n",
      "\n",
      "Epoch 127/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0836 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0821\n",
      "\n",
      "Epoch 128/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 40us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0841 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0749\n",
      "\n",
      "Epoch 129/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 40us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0837 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0790\n",
      "\n",
      "Epoch 130/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 42us/step - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0841 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0805\n",
      "\n",
      "Epoch 131/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 41us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0836 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0824\n",
      "\n",
      "Epoch 132/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0834 - val_loss: 0.0119 - val_mean_squared_error: 0.0119 - val_mean_absolute_error: 0.0842\n",
      "\n",
      "Epoch 133/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0836 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0770\n",
      "\n",
      "Epoch 134/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 33us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0836 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0808\n",
      "\n",
      "Epoch 135/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0833 - val_loss: 0.0126 - val_mean_squared_error: 0.0126 - val_mean_absolute_error: 0.0895\n",
      "\n",
      "Epoch 136/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0837 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0801\n",
      "\n",
      "Epoch 137/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 39us/step - loss: 0.0131 - mean_squared_error: 0.0131 - mean_absolute_error: 0.0833 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0801\n",
      "\n",
      "Epoch 138/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 41us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0834 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0746\n",
      "\n",
      "Epoch 139/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0835 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0819\n",
      "\n",
      "Epoch 140/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 37us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0837 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0784\n",
      "\n",
      "Epoch 141/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 32us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0834 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0815\n",
      "\n",
      "Epoch 142/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0837 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0757\n",
      "\n",
      "Epoch 143/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0835 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0787\n",
      "\n",
      "Epoch 144/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0835 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0830\n",
      "\n",
      "Epoch 145/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0832 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 146/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 35us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0836 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0762\n",
      "\n",
      "Epoch 147/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 32us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0835 - val_loss: 0.0117 - val_mean_squared_error: 0.0117 - val_mean_absolute_error: 0.0822\n",
      "\n",
      "Epoch 148/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0133 - mean_squared_error: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0116 - val_mean_squared_error: 0.0116 - val_mean_absolute_error: 0.0817\n",
      "\n",
      "Epoch 149/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 36us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0834 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0793\n",
      "\n",
      "Epoch 150/150\n",
      "10448/10448 [==============================]10448/10448 [==============================] - 0s 34us/step - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0834 - val_loss: 0.0115 - val_mean_squared_error: 0.0115 - val_mean_absolute_error: 0.0763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=150, batch_size=50,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mean_squared_error', 'val_mean_absolute_error', 'loss', 'mean_squared_error', 'mean_absolute_error'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIGrY+glaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103f16a3358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xnew = np.array([[1,-1,100,7,4,0,1,3]])\n",
    "xnew = scaler_x.transform(xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ynew = model.predict(xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ynew = scaler_y.inverse_transform(ynew)\n",
    "xnew = scaler_x.inverse_transform(xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[  1.  -1. 100.   7.   4.   0.   1.   3.],Predicted = [305.03143]\n"
     ]
    }
   ],
   "source": [
    "print(\"x=%s,Predicted = %s\"%(xnew[0],ynew[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
